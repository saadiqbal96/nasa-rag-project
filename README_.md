ğŸš€ NASA RAG Chat Project
Retrieval-Augmented Generation with ChromaDB, OpenAI & RAGAS

Author: Saad Iqbal
Repository: nasa-rag-project
Project Type: End-to-End RAG System with Evaluation and Streamlit UI

ğŸŒŒ Project Overview

This project implements a fully-functional Retrieval-Augmented Generation (RAG) system for NASA mission documents (Apollo 11, Apollo 13, Challenger).

The system follows the complete RAG lifecycle:

Document ingestion â†’ chunking â†’ OpenAI embeddings â†’ ChromaDB storage â†’ retrieval â†’ grounded LLM responses â†’ batch evaluation

All design decisions are explicitly aligned with the Udacity project rubric and prior reviewer feedback.

âœ… Rubric-Aligned Features (At a Glance)

âœ” ChromaDB persistent vector store

âœ” OpenAI embedding model (text-embedding-3-small)

âœ” Configurable chunk size & overlap (CLI flags)

âœ” Document update modes: skip / update / replace

âœ” Metadata-aware retrieval (mission, source, category)

âœ” System-prompt-based LLM grounding

âœ” Conversation history management

âœ” Batch evaluation with RAGAS-style metrics

âœ” Streamlit chat interface

ğŸ§  Core Architecture

Text Files
   â†“
Configurable Chunking
   â†“
OpenAI Embeddings
   â†“
ChromaDB (Persistent Collection)
   â†“
Metadata-Filtered Retrieval
   â†“
Grounded LLM Responses
   â†“
Batch Evaluation (RAGAS-style)

ğŸ“ Repository Structure
.

â”œâ”€â”€ chat_.py                   # Streamlit chat UI

â”œâ”€â”€ embedding_pipeline_.py     # ChromaDB + OpenAI embedding pipeline

â”œâ”€â”€ RAG_CLIENT_.py             # Retrieval + context construction

â”œâ”€â”€ LLM_CLIENT_.py             # System-prompted LLM client

â”œâ”€â”€ ragas_evaluator_.py        # Evaluation metrics (relevancy, faithfulness)

â”œâ”€â”€ ragas_batch_eval.py        # Batch evaluation runner

â”œâ”€â”€ evaluation_dataset.jsonl   # Evaluation test set (â‰¥5 questions)

â”œâ”€â”€ ragas_report.json          # Generated evaluation report

â”œâ”€â”€ chroma.sqlite3             # Persistent ChromaDB store

â”œâ”€â”€ AS13_TEC_.txt              # NASA Apollo 13 technical transcript

â”œâ”€â”€ README.md

â””â”€â”€ gitignore.txt

ğŸ§© Key Implementation Details

ğŸ”¹ 1. Configurable Chunking (Rubric Critical)

Chunk size and overlap are runtime-configurable

No hard-coded constants

--chunk-size 500
--chunk-overlap 100


Validation ensures overlap is always smaller than chunk size.

ğŸ”¹ 2. OpenAI Embeddings + ChromaDB

Uses OpenAI embeddings (text-embedding-3-small)

Stored in a persistent ChromaDB collection

Supports:

--chroma-dir

--collection-name

--stats-only mode

ğŸ”¹ 3. Update Modes for Existing Documents

Embedding pipeline supports:

skip â†’ ignore existing documents

update â†’ update existing chunks

replace â†’ delete and re-ingest

--update-mode skip|update|replace

ğŸ”¹ 4. Retrieval & Context Construction

Retriever provides:

Top-K configurable retrieval

Optional mission filtering

Score-sorted results

Deduplication

Clean, single context block

Example context header:

[Apollo 13 â€¢ AS13_TEC â€¢ Technical â€¢ Score: 0.412]

ğŸ”¹ 5. LLM Grounding & System Prompt

The LLM client uses a strict system prompt:

Positions the model as a NASA mission expert

Forces answers to rely only on retrieved context

Requires explicit source citations

States uncertainty when context is insufficient

Conversation history is maintained and trimmed for multi-turn chat.

ğŸ”¹ 6. Evaluation & Batch Metrics

Includes:

evaluation_dataset.jsonl (â‰¥5 Apollo-13 questions)

Batch evaluation script

Metrics include:

Relevancy

Faithfulness

Outputs:

ragas_report.json

ğŸ’¬ Example Questions

Try asking:

â€œWhat caused the Apollo 13 oxygen tank explosion?â€

â€œAt what altitude were the Apollo 13 parachutes first visible?â€

â€œWhat did the crew report immediately after the explosion?â€

â€œWhat communication difficulties occurred during re-entry?â€

ğŸ–¥ï¸ Running the Project
1ï¸âƒ£ Build / Update the Vector Store

python embedding_pipeline_.py \

  --data-path . \

  --openai-key $OPENAI_API_KEY \
  
  --chroma-dir ./chroma_db_openai \
  
  --collection-name nasa_space_missions_text \
  
  --chunk-size 500 \
  
  --chunk-overlap 100 \
  
  --update-mode skip

2ï¸âƒ£ Launch the Chat UI
streamlit run chat_.py

3ï¸âƒ£ Run Batch Evaluation
python ragas_batch_eval.py

ğŸ§ª Why This Meets the Rubric

This submission directly addresses prior reviewer feedback by:

Using ChromaDB instead of FAISS

Using OpenAI embeddings instead of local MiniLM

Making chunking configurable at runtime

Adding update modes

Adding a system prompt + conversation memory

Producing batch evaluation output

Aligning README claims with actual code behavior

ğŸ‘¤ Author

Saad Iqbal
nasa-rag-project
